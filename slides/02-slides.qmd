---
title: "POLS 1600"
subtitle: "Data Visualization"
date: last-modified
date-format: "[Updated ]MMM D, YYYY"
format: 
  revealjs:
    theme: brownslides.scss
    logo: images/pols1600_hex.png
    footer: "POLS 1600"
    multiplex: false
    transition: fade
    slide-number: c
    incremental: true
    center: false
    menu: true
    scrollable: true
    highlight-style: github
    progress: true
    code-overflow: wrap
    # include-after-body: title-slide.html
    title-slide-attributes:
      align: left
      data-background-image: images/pols1600_hex.png
      data-background-position: 90% 50%
      data-background-size: 40%
filters:
    - openlinksinnewpage
execute: 
  eval: true
  echo: true
  warning: false

    # title-slide-attributes:
    #   data-background-image: ../../assets/stat20-hex-bg.png
    #   data-background-size = contain
---

```{r}
#| echo: false
#| warning: false 
#| message: false

library(tidyverse)
library(labelled)
library(haven)
library(DeclareDesign)
library(easystats)

# Class survey from previous week

df <- haven::read_spss("../files/data/class_surveys/wk01.sav")
```

# {{< fa map-location>}} Overview {.inverse}

## Class Plan {.smaller}

- Announcements
-   Setup (5 minutes)
-   Review
    -   Troubleshooting Errors (5 min)
    -   Data wrangling in R (20 min)
    -   Descriptive Statistics (10 min)
-   Data Visualization (40 min)
    -   The grammar of graphics
    -   Basic plots to describe:
        -   Distributions
        -   Associations
        
## Concept check {.smaller}

Suppose you want to do the following, what function or functions would you use:

-   Read data into `R`
-   Look at the data to get a high level overview of its structure
-   Subset or filter the data to include just observations with certain values
-   Select specific columns from data
-   Add new columns to the data
-   Summarize multiple values by collapsing them into a single value
-   Doing some function group-by-group?        

## Announcements

- Submit tutorials from last week for full credit by this Sunday.

- Groups for the course assigned next week

# {{< fa code>}} Tutorials {.inverse}


## Tutorials

Once you've done the following

```{r}
#| eval = F

remotes::install_github("kosukeimai/qss-package", build_vignettes = TRUE)
remotes::install_github("rstudio/learnr")
remotes::install_github("rstudio-education/gradethis")
remotes::install_github("PaulTestaBrown/qsslearnr")
```

You can see the available problem sets by running the following code in your console:

```{r}
learnr::run_tutorial(package = "qsslearnr")
```

## Tutorials

And start a specific tutorial by running:

```{r}
#| eval = F
learnr::run_tutorial("00-intro", package = "qsslearnr")
```


:::{.callout-important}

Please upload tutorials 00-intro and 01-measurement1 to [Canvas](https://canvas.brown.edu/courses/1098409/assignments) by Friday

:::

# {{< fa code>}} Setup {.inverse}

## Setup for today

## Libraries

This week we'll use the following libraries.

```{r}
#| label = "packages",
#| cache = F
the_packages <- c(
  ## R Markdown
  "tinytex", "kableExtra",
  
  ## Tidyverse
  "tidyverse","lubridate", "forcats", "haven","labelled",
  
  ## Extensions for ggplot
  "ggmap","ggrepel", "ggridges", "ggthemes","ggpubr",
  "GGally",
  
  # Data 
  "maps","mapdata","DT"
)
the_packages
```

## Installing and loading new packages

Next we'll create a function called `ipak` (thanks [Steven](https://gist.github.com/stevenworthington/3178163)) which:

-   Takes a list of packages (`pkg`)
-   Checks to see if these packages are installed
-   Installs any new packages
-   Loads all the packages so we can use them

```{r}
#| label = "ipak"
ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}

```

Again, run this code on your machines

## Installing and loading new packages {.smaller}

Finally, let's use `ipak` to install and load `the_packages`

What should we replace `some_function` and `some_input` with to do this?

```{r}
#| eval = F
some_function(some_input)
```

. . .

```{r}
#| label = "libraries",
#| cache = F
ipak(the_packages)
```

-   `R` may ask you to install a package's dependencies (other packages your package needs). Try entering the number `1` into your console
-   `R` may tell you need to restart `R` Try saying yes. If it doesn't start downloading, say no
-   `R` may then ask if you want to compile some packages from source. Type `Y` into your console. If this doesn't work, try again, but this time type `N` when asked

## Loading the Covid-19 Data

Let's load the Covid-19 data we worked with last week:

```{r}
#| label = "covid"
load(url("https://pols1600.paultesta.org/files/data/covid.rda"))

```

# {{< fa magnifying-glass>}} Troubleshooting Errors {.inverse}

## 

![](https://imgs.xkcd.com/comics/wisdom_of_the_ancients.png)

[XKCD](https://xkcd.com/979)

## Two kinds of errors: {.smaller}

-   **Syntactic**
    -   R doesn't understand how to run your code
    -   Most common, easy to fix (eventually...)

. . .

-   **Semantic**
    -   R runs your code but doesn't give you the expected result
    -   Less common, harder to fix

. . .

Most errors happen because R is looking for something that isn't there.

More discussion [here](https://github.com/noamross/zero-dependency-problems/blob/master/misc/stack-overflow-common-r-errors.md) and [here](https://blog.revolutionanalytics.com/2015/03/the-most-common-r-error-messages.html)

## Common Syntactic Errors

-   Unmatched parentheses or brackets

-   Misspelled a name

-   Forgot a comma

-   Forgot to install a package or load a library

-   Forgot to set the working directory/path to a file you want R to use.

-   Tried to select a column or row that doesn't exist

## Fixing Syntactic Errors {.smaller}

-   R Studio's script editor will show a red circle with a white x in next to a line of code it thinks has an error in it.

-   Have someone else look at your code (Fresh eyes, [paired programming](https://en.wikipedia.org/wiki/Pair_programming))

-   Copy and paste the "general part" of error message into Google/ChatGPT

-   Knit your document after each completed code chunk

    -   This will run the code from top to bottom, and stop when it encounters an error
    -   Try commenting out the whole chunk, and then uncommenting successive lines of code

-   Be patient. Don't be hard are yourself. Remember, errors are portals of discovery.

## Semantic Errors

-   Your code runs, but doesn't produce what you expected.
-   Less common; can be harder to identify and fix
-   One example: Two packages have a function with the same name that do different things

. . .

```{r}
# dplyr::summarize
# Hmisc::summarize
```

## Semantic Errors {.smaller}

-   Some general solutions/practices to avoid semantic errors:
    -   Specify the package and the function you want: `package_name::function_name()`
    -   Write helpful comments in your code.
    -   Include "sanity" checks in your code.
    -   If a function should produce an output that's a data.frame, check to see if it is a data frame

. . .

```{r}
#| eval: false
# Here's some pseudo code:

# I expect my_function produces a data frame
x <- my_function(y) 

# Check to see if x is a data frame
# If x is not a data frame, return an Error
stopifnot(is.data.frame(x))
```

# {{< fa magnifying-glass >}} Data Wrangling in R {.inverse}

## Why do we need to "wrangle" data

-   Rarely, if ever, do we get data in the exact format we need.

-   Instead, before we can get to work, we often need to transform our data in various ways

-   Sometimes called:

    -   Data cleaning/recoding
    -   Data wrangling
    -   Data carpentry

-   The end goal is the same: [make messy data tidy]{.blue}

## Tidy data

-   Every column is a [variable]{.blue}.

-   Every row is an [observation]{.blue}.

-   Every cell is a [single value]{.blue}

## Tools for transforming our data {.smaller}

Last week we used the following functions:

-   `read_csv()` and `data()` to read and load data in R

-   logical operators like `&`, `|`, `%in%` `==`, `!=`, `>`,`>=`,`<`,`<=` to make comparisons

-   the pipe command `%>%` to "pipe" the output of one function into another

-   `filter()` to pick observations (rows) by their values

-   `arrange()` to reorder rows

-   `select()` to pick variables by their names

-   `mutate()` and `case_when()` command to create new variables in our data set

-   `summarise()` to collapse many values into a single value (like a mean or median)

-   `group_by()` to apply functions like `mutate()` and `summarise()` on a group-by-group basis

## Common functions for transforming data

All of these "verb" functions from the `dplyr` package (e.g. `filter()`,`mutate()`) follow a similar format:

1.  Their first argument is a data frame
2.  The subsequent arguments tell R what to do with the data frame, using the variable names (without quotes)
3.  The output is a new data frame

[More](https://r4ds.had.co.nz/transform.html)

## You trying to get the `%>%`?

![](https://a.espncdn.com/combiner/i?img=/i/headshots/nba/players/full/2444.png){fig-align="center"}

## The pipe command `%>%` {.smaller}

-   The pipe command is way of "chaining" lines of code together, piping the results of one `tidyverse` function into the next function.

-   The pipe command works because these functions always expect a data frame as their first argument, and always produce a data frame as their output.


## The pipe command `%>%`

```{r}
#| eval: false
#| code-line-numbers: "2|8"

summarise(
  data = df,
  mean = mean(var1, na.rm = T),
  median = median(var1, na.rm = T)
 )
# Rewrite with a pipe:

df %>% 
  summarize(
    mean = mean(var1, na.rm = T),
    median = median(var1, na.rm = T)    
  )



```

## Wrangling the Covid-19 data

To work with the Covid-19 data we did the following:

-   Subsetted/Filtered the data to exclude US Territories
-   Created new variables from existing variables in the data to use in our final analysis

## Wrangling the Covid-19 data {.smaller}

Specifically, we did the following:

1.  Created an object called `territories` that is a vector containing the names of U.S. territories
2.  Created a new dataframe, called `covid_us`, by filtering out observations from the U.S. territories
3.  Created a `state` variable that is a copy of the `administrative_area_level_2`
4.  Created a variable called `new_cases` from the `confirmed`. Create a variable called `new_cases_pc` that is the number of new Covid-19 cases per 100,000 citizens
5.  Created a variable called `face_masks` from the `facial_coverings` variable.
6.  Calculated the average number of new cases, by different levels of `face_masks`

. . .

Let's take some time to make sure we understand everything that was happening.

## Created an object called `territories`

```{r}
# - 1. Create territories object

territories <- c(
  "American Samoa",
  "Guam",
  "Northern Mariana Islands",
  "Puerto Rico",
  "Virgin Islands"
  )
```

-   The object `territories` now exists in our environment.

## Created a new dataframe, called `covid_us`

::: nonincremental
::: panel-tabset
## Task

-   Use the `filter()` command to select only the rows where the `administrative_area_level_2` is not (`!`) in (`%in%`) the `territories` object

## Code

```{r}
# - 2. Create covid_us data frame
# How many rows and columns in covid
dim(covid)

# Filter out obs from US territories
covid_us <- covid %>%
  filter(!administrative_area_level_2 %in% territories)

# covid_us should have fewer rows than covid
dim(covid_us)
```
:::
:::

## Created a variable called `state`

::: nonincremental
::: panel-tabset
## Task

Copy `administrative_area_level_2` into a new variable called `state`

::: callout-note
Note that we have to [save the output]{.blue} of mutate back into `covid_us` for our `state` to exist as new column in `covid_us`
:::

## Code

```{r}
#| code-line-numbers: "3|4"
dim(covid_us)
covid_us %>%
  mutate(
    state = administrative_area_level_2
  ) -> covid_us
dim(covid_us)
names(covid_us)[48]
```
:::
:::

## Created a variable called `state` {.smaller}

Now there's a new column in `covid_us` called `state`, that we can access by calling `covid_us$state`

```{r}
covid_us$state[1:5] # Just show first 5 observations
```

. . .

We could have done the same thing in "Base" R

```{r}
#| eval: false
covid_us$state <- covid_us$administrative_area_level_2
```

. . .

Why didn't we?

-   Consistent preference for `tidyverse` \> `base R`
-   Saves time when recoding lots of variables
-   `mutate()` plays nicely with functions like `group_by()`

## Create a variable called `new_cases` from the `confirmed` variable

The `confirmed` variable contains a running total of confirmed cases in a given state on a given day.

[Vizualing data]{.blue} helps us understand how we might need to transform our data

## Visualize `confirmed` variable for Rhode Island

::: panel-tabset
## Code

```{r}
options(scipen = 999) # No scientific notation
covid_us %>% 
  filter(state == "Rhode Island") %>% 
  ggplot(aes(
    x = date,
    y = confirmed
  ))+
  geom_point()+
  theme_bw() +
  labs(title = "Total Covid-19 cases in Rhode Island",
       y = "Total Cases",
       x = "Date") -> fig_ri_covid

```

## Plot

```{r}
#| echo: false
fig_ri_covid
```

## Data

```{r}
#| echo: false
ri_df <- covid_us %>% 
  filter(state == "Rhode Island") %>% 
  select(state,date,confirmed) %>% 
  na.omit()

DT::datatable(ri_df)
```
:::

## Create a variable called `new_cases` from the `confirmed` variable {.smaller}

::: panel-tabset
## Task

Take the difference between a given day's value of `confirmed` and yesterday's value of `confirmed` to create a measure of `new_cases` on a given date for each state

::: callout-note
-   Use `lag()` to shift values in a column down one row in the data
-   Use `group_by()` to respect the state-date structure of the data
:::

## Code

```{r}
covid_us %>%
  dplyr::group_by(state) %>%
  mutate(
    new_cases = confirmed - lag(confirmed)
  ) -> covid_us
```

## Data

```{r}
#| echo: false
lag_df <- covid_us %>%
    filter(date >= "2020-04-01" & date < "2020-04-07")%>%
  select(state, date, confirmed)%>%
  ungroup() %>% 
  mutate(
    lag_no_grp = lag(confirmed),
    new_cases_no_grp = confirmed - lag(confirmed)
  )%>%
  group_by(state) %>%

  mutate(
    lag_grp = lag(confirmed),
    new_cases = confirmed - lag(confirmed)
  ) %>% 
  select(state,date, confirmed, lag_no_grp,lag_grp,new_cases_no_grp, new_cases)
DT::datatable(lag_df)
```
:::

## Create a variable called `new_cases_pc` {.smaller}

::: nonincremental
::: panel-tabset
## Task

-   Scale `new_cases` by `population` to create a per capita measure (`new_cases_pc`)

::: callout-note
We can create [multiple]{.blue} variables in a single `mutate()` by separating lines of code with a `,`
:::

## Code - Wrangling

```{r}
covid_us %>%
  mutate(
    state = administrative_area_level_2,
  ) %>%
  dplyr::group_by(state) %>%
  mutate(
    new_cases = confirmed - lag(confirmed),
    new_cases_pc = new_cases / population *100000
    ) ->covid_us
```

## Code - Checking

```{r}
# Check recoding
covid_us %>% 
  # Look at two states
  filter(state == "Rhode Island" | state == "New York") %>% 
  # In a small date range
  filter(date > "2021-01-01" & date < "2021-01-05") %>% 
  # Select only the columns we want
  select(state, date, new_cases, new_cases_pc) -> hlo_df
# save to object hlo_df
```

## Data

```{r}
hlo_df
```
:::
:::

## Created a variable called `face_masks` {.smaller}

::: nonincremental
::: panel-tabset
## Task

Create a variable called `face_masks` from the `facial_coverings` that describes the face mask policy experienced by most people in a given state on a given date.

::: callout-note
-   Use `case_when()` inside of `mutate()` to create a variable that takes certain values when certain logical statements are true
-   Seting the `levels = c(value1, value2, etc.)` argument in `factor()` lets us control the [ordering]{.blue} of categorical/character data.
:::

## HLO

Recall, that the `facial_coverings` variable took on range of substantive values from 0 to 4, but empirically could take both positve and negative values

```{r}
table(covid_us$facial_coverings)
```

## Code

```{r}
covid_us %>%
mutate(
    face_masks = case_when(
      facial_coverings == 0 ~ "No policy",
      abs(facial_coverings) == 1 ~ "Recommended",
      abs(facial_coverings) == 2 ~ "Some requirements",
      abs(facial_coverings) == 3 ~ "Required shared places",
      abs(facial_coverings) == 4 ~ "Required all times",
    ) %>% factor(.,
      levels = c("No policy","Recommended",
                 "Some requirements",
                 "Required shared places",
                 "Required all times")
    ) 
    ) -> covid_us
```

## Check

```{r}
covid_us%>%
  filter(state == "Illinois", date > "2020-9-28") %>%
  select(state, date, facial_coverings, face_masks) %>% 
  slice(1:5)
```
:::
:::

## {{< fa face-meh >}} Addtional recoding

In last week's lab, we also added the following

```{r}
covid_us %>%
  mutate(
    year = year(date),
    month = month(date),
    year_month = paste(
      year, 
      str_pad(month, width = 2, pad=0), 
      sep = "-"
      ),
    percent_vaccinated = people_fully_vaccinated/population*100  
    ) -> covid_us
```

## {{< fa face-meh >}} Working with dates

R treats dates differently

```{r}
covid_us$date[1:3]
class(covid_us$date)
```

If R knows a variable is a date, we can extract components of that date, using functions from the `lubridate` package

```{r}
year(covid_us$date[1:3])
month(covid_us$date[1:3])
```

## {{< fa face-meh >}} The `str_pad()` and `paste()` function

-   The `str_pad()` function lets us 'pad' strings so that they're all the same width

```{r}
month(covid_us$date[1:3])
str_pad(month(covid_us$date[1:3]), width=2, pad = 0)
```

-   The `paste` function lets us paste objects together.

```{r}
paste(year(covid_us$date[1:3]),
      str_pad(month(covid_us$date[1:3]), width=2, pad = 0),
      sep = "-"
      )
```

## Summarizing the averge number of `new_cases` by `face_mask` policy {.smaller}

::: panel-tabset
## Task

Calculate the mean (average) number of `new_cases` of Covid-19 when each type of `face_mask` policy was in effect

::: callout-note
-   The `group_by()` command will do each calculation inside of `summarise()` for each level of the grouping variable
:::

## Code

```{r}
covid_us %>%
  filter(!is.na(face_masks)) %>%
  group_by(face_masks) %>%
  summarize(
    new_cases_pc = mean(new_cases_pc, na.rm=T)
  ) -> face_mask_summary

```

## Results

```{r}
face_mask_summary
```
:::

## Summarizing the averge number of `new_cases` by `face_mask` policy by month {.smaller}

:::: panel-tabset

## Task

Calculate the mean (average) number of `new_cases` of Covid-19 when each type of `face_mask` policy was in effect for each `year_month` in our dataset

::: callout-note
-   The `group_by()` command can group on [multiple]{.blue} variables
:::

## Code

```{r}
covid_us %>%
  group_by(face_masks, year_month) %>%
  summarize(
    new_cases_pc = mean(new_cases_pc, na.rm=T)
  ) -> cases_by_month_and_policy

```

## Results

```{r}
cases_by_month_and_policy

# In base R:
mean(
  covid_us$new_cases_pc[
    covid_us$face_masks == "No policy" &
      covid_us$year_month == "2020-01"], na.rm = T)

```

::::

## {{< fa lightbulb >}} Concept check {.smaller}

Suppose you want to do the following, what function or functions would you use:

-   Read data into `R`
-   Look at the data to get a high level overview of its structure
-   Subset or filter the data to include just observations with certain values
-   Select specific columns from data
-   Add new columns to the data
-   Summarize multiple values by collapsing them into a single value
-   Doing some function group-by-group?

## Concept check {.smaller}

Suppose you want to do the following, what function or functions would you use:

-   Read data into `R`
    -   `read_xxx()` (tidy), `read.xxx()` (base)
-   Look at the data to get a high level overview of its structure
    -   `head()`, `tail()`, `glimpse()`, `table()`, `summary()`, `View()`
-   Filter the data to include just observations with certain values
    -   `data %>% filter(x > 0)`, `data[data$x > 0]`, `subset(data, x > 0)`
-   Select specific columns from data
    -   `data$variable`, `data %>% select(variable1, variable2)`, `data[,c("x1","x2")]`
-   Add new columns to the data
    -   `data %>% mutate(x = y/10)` `data$x <- data$y/10`
-   Summarize multiple values by collapsing them into a single value
    -   `data %>% summarise(x_mn = mean(x, na.rm=T))`
-   Doing some function group-by-group?
    -   `data %>% group_by(g) %>% summarise(x_mn = mean(x, na.rm=T))`

## Concept check {.smaller}

Should you know exactly how to do all of this?

. . .

**NO! Of course not. For Pete's sake, Paul, It's only the second week**

. . .

Will you learn how to do much of this?

. . .

**Maybe, but I'm feeling pretty overwhelmed...**

. . .

How will you learn how do these things?

. . .

**With lots of practice, patience, and repetition motivated by a sense that these skills will help me learn about things I care about**

## Advice on learning how to code {.smaller}

-   It takes lots of practice and lots of errors
    -   Break long blocks of code into individual steps to see what's happening
-   Create code chunks and FAFO
    -   Just clean up when you're done...
-   Only dumb question is one you don't ask
-   Google, Stack Exchange are your friends
-   Try writing out in comments what you want to do in code
-   Learn to recognize patterns in the questions/tasks I give you:
    -   Copy and paste code I give
    -   Change one thing
    -   Fix the error
    -   Adapt code from class to do a similar thing
-   Learning to code is much less painful when you have a reason to do it
    -   Let me know what interests you

# {{< fa magnifying-glass>}} Descriptive Statistics {.inverse}

## Descriptive Statistics {.smaller}

When social scientists talk about descriptive inference, we're trying to summarize our data and make claims about what's *typical* of our data

-   What's a typical value
    -   [Measures of central tendency]{.blue}
    -   mean, median, mode
-   How do our data vary around typical values
    -   [Measures of dispersion]{.blue}
    -   variance, standard deviation, range, percentile ranges
-   How does variation in one variable relate to variation in another
    -   [Measures of association]{.blue}
    -   covariance, correlation

## Using R to Summarize Data {.smaller}

Here are some common ways of summarizing data and how to calculate them with `R`

```{r}
#| echo = F
summary_table <- tibble::tribble(
  ~Description, ~Usage,
  "sum", "sum(x)",
  "minimum", "min(x)",
  "maximum", "max(x)",
  "range", "range(x)",
  "mean", "mean(x)",
  "median", "median(x)",
  "percentile", "quantile(x)",
   "variance", "var(x)",
  "standard deviation", "sd(x)",
  "rank", "rank(x)"
)
```

```{r}
#| echo = FALSE,
#| results = "asis"
knitr::kable(summary_table, format = "html")
```

All of these functions have an argument called `na.rm=F`. If your data have missing values, you'll need to set `na.rm=F` (e.g. `mean(x, na.rm=T)`)

## What you need to know for POLS 1600{.smaller}

**Measures of typical values**

-   **Means** (`mean()`) all the time
-   **Medians** (`median()`) useful for describing distributions of variables particularly those with [extreme values]{.blue}
-   **Mode** useful for characterizing [categorical]{.blue} data

## What you need to know for POLS 1600{.smaller}



**Measures of typical variation**

-   `var()` important for quantifying uncertainty, but rarely will you be calculating this directly
-   `sd()` a good summary of a [typical change]{.blue} in the data.
-   `range()`, `min()`, `max()` useful for exploring data, detecting outliers and potential values that need to be recoded

## What you need to know for POLS 1600{.smaller}


**Measures of association**

-   Covariance (`var()`) central to describing relationships but generally not something you'll calculate or interpret directly
-   Correlation (`cor()`) useful for describing \[bivariate\] relationships (positive or negative relationships).

## What you don't really need to know for POLS 1600 {smaller}

We won't spend much time on the formal definitions, math, and proofs

$$
\bar{x}=\frac{1}{n}\sum_{i=1}^n x_i 
$$

$$
M_x = X_i : \int_{-\infty}^{x_i} f_x(X)dx=\int_{x_i}^\infty f_x(X)dx=1/2
$$

. . .

Useful eventually. Not necessary right now.

# {{< fa lightbulb >}} Data Visualization: The Grammar of Graphics {.inverse}

## Data visualizaiton

Data visualization is an incredibly valuable tool that helps us to

-   Explore data, uncovering new relationships, as well as potential problems
-   Communicate our results clearly and precisely

Take a look at how the [BBC](https://medium.com/bbc-visual-and-data-journalism/how-the-bbc-visual-and-data-journalism-team-works-with-graphics-in-r-ed0b35693535) uses R to produce its graphics

## Data visualization

Today, we will:

-   Introduce the `grammar of graphics`
-   Learn how to apply this grammar with `ggplot()`
-   Introduce basic plots to describe
    -   Univariate distributions
    -   Bivariate relations

## The Grammar of Graphics {.smaller}

Inspired by [Wilkinson (2005)](https://link.springer.com/book/10.1007/0-387-28695-0)

> A statistical graphic is a mapping of `data` variables to `aes` thetic attributes of `geom` etric objects.

At a minimum, a graphic contains three core components:

-   `data:` the dataset containing the variables of interest.
-   `aes`: aesthetic attributes of the geometric object. For example, x/y position, color, shape, and size. Aesthetic attributes are mapped to variables in the dataset.
-   `geom:` the geometric object in question. This refers to the type of object we can observe in a plot For example: points, lines, and bars.

[Ismay and Kim (2022)](https://moderndive.com/2-viz.html#grammarofgraphics)

## Seven Layers of Graphics {background-image="https://miro.medium.com/max/1400/1*ZdQ9wFwaA214ABiib7AygQ.png" background-size="contain"}

[Kesari (2018)](https://gkesari.medium.com/my-talk-on-grammar-of-graphics-the-secret-sauce-of-powerful-data-stories-3da618cf1bbf)

## The grammar of graphics in R

In R, we'll implement this grammar of graphics using the `ggplot` package

-   Let's take a look at your feedback to last week's survey and see how we can visualize some of the in formation you provided


# {{< fa bullhorn >}} Feedback {.inverse}

## What we liked {.smaller}

```{r}
#| echo: false

DT::datatable(df %>% 
                select(like))
```

## What we disliked {.smaller}

```{r}
#| echo: false

DT::datatable(df %>% 
                select(dislike))
```

## 

```{r}
#| echo: false

df %>%
  filter(!is.na(trip)) %>% 
  mutate(
    Playlist = str_wrap(forcats::as_factor(trip),20)
  ) %>%
  ggplot(aes(x = Playlist,
             fill = Playlist))+
  geom_bar(stat = "count")+
  scale_fill_brewer(guide="none")+
  coord_flip()+
  labs(title = "Who controls the playlist",
       x= "",
       y = "")+
  theme_bw()
```

## Building that figure

1.  Look at the raw data
2.  Recode the raw data
3.  Make a basic plot, telling R the `data`, `aes`thetics, `geom`etries, and `stat`istics I want it to plot
4.  Tinker with the data and plot's scales, coordinates, labels and theme to make the figure look better

## 1. Look at the raw data

```{r}
df$trip
```

## 2. Recode the raw data

```{r}
#| code-line-numbers: "3"
df %>%
  mutate(
    Playist = forcats::as_factor(trip)
 )%>%
  select(Playist)
```

## 3. Make a basic plot

::: panel-tabset
## Code

```{r}

df %>% #<< Raw data
  mutate(
    Playlist =forcats::as_factor(trip)
  ) %>% # Transformed data
  ggplot(aes(x = Playlist, # Aesthetics
             fill = Playlist))+
  geom_bar( # Geometry
    stat = "count" # Statistic
    ) -> fig_roadtrip
```

## Plot

```{r}
fig_roadtrip
```
:::

## 4.1 Tinker with data

::: panel-tabset
## Code

```{r}
#| code-line-numbers: "2|4"
 
df %>%
  filter(!is.na(trip)) %>% 
  mutate(
    Playlist =str_wrap(forcats::as_factor(trip),20)
  ) %>%
  ggplot(aes(x = Playlist,
             fill = Playlist))+
  geom_bar(stat = "count") -> fig_roadtrip

```

## Plot

```{r}
#| echo: false
fig_roadtrip

```
:::

## 4.2 Tinker with `fill` aesthetic

::: panel-tabset
## Code

```{r}
#| code-line-numbers: "9"

df %>%
  filter(!is.na(trip)) %>% 
  mutate(
    Playlist =str_wrap(forcats::as_factor(trip),20)
  ) %>%
  ggplot(aes(x = Playlist,
             fill = Playlist))+
  geom_bar(stat = "count")+
  scale_fill_brewer() -> fig_roadtrip
```

## Plot

```{r}
#| echo: false
fig_roadtrip

```
:::

## 4.3 Tinker with coordinates

::: panel-tabset
## Code

```{r}
#| code-line-numbers: "10"

df %>%
  filter(!is.na(trip)) %>% 
  mutate(
    Playlist =str_wrap(forcats::as_factor(trip),20)
  ) %>%
  ggplot(aes(x = Playlist,
             fill = Playlist))+
  geom_bar(stat = "count")+
  scale_fill_brewer() +
  coord_flip() -> fig_roadtrip
```

## Plot

```{r}
#| echo: false
fig_roadtrip

```
:::

## 4.4 Tinker with labels

::: panel-tabset
## Code

```{r}
#| code-line-numbers: "9|11-13"

df %>%
  filter(!is.na(trip)) %>% 
  mutate(
    Playlist =str_wrap(forcats::as_factor(trip),20)
  ) %>%
  ggplot(aes(x = Playlist,
             fill = Playlist))+
  geom_bar(stat = "count")+
  scale_fill_brewer(guide="none")+
  coord_flip()+
  labs(title = "Who controls the playlist",
       x= "",
       y = "")-> fig_roadtrip
```

## Plot

```{r}
#| echo: false
fig_roadtrip

```
:::

## 4.4 Tinker with theme

::: panel-tabset
## Code

```{r}
#| code-line-numbers: "14"

df %>%
  filter(!is.na(trip)) %>% 
  mutate(
    Playlist =str_wrap(forcats::as_factor(trip),20)
  ) %>%
  ggplot(aes(x = Playlist,
             fill = Playlist))+
  geom_bar(stat = "count")+
   scale_fill_brewer(guide="none")+
  coord_flip()+
  labs(title = "Who controls the playlist",
       x= "",
       y = "")+
  theme_bw() -> fig_roadtrip
```

## Plot

```{r}
#| echo: false
fig_roadtrip

```
:::

## The final code

```{r}
#| echo: true
#| eval: false
df %>%
  filter(!is.na(trip)) %>% 
  mutate(
    Playlist =str_wrap(forcats::as_factor(trip),20)
  ) %>%
  ggplot(aes(x = Playlist,
             fill = Playlist))+
  geom_bar(stat = "count")+
   scale_fill_brewer(guide="none")+
  coord_flip()+
  labs(title = "Who controls the playlist",
       x= "",
       y = "")+
  theme_bw() -> fig_roadtrip
```

# {{< fa lightbulb >}} DataViz: Describing Distributions and Associations {.inverse}

## Describing Distributions and Associations

-   In the remaining slides, we' see how to visualize some distributions and associations in the Covid data using:

    -   barplots
    -   histograms
    -   density plots
    -   boxplots
    -   line plots
    -   scatter plots

## General advice for making figures

-   Think through conceptually how you want to figure to look
    -   Draw it out by hand
-   Make a basic plot and iterate
-   Use `summarize()` and other data wrangling skills to transform data for plotting
-   Use `factor()` and related functions to control order of labels on axis
- Use google to figure out arcane options of `ggplot`
-   Don't let the perfect be the enemy of the good

## Barplots {.smaller}

::: panel-tabset
## Question

What was the most common face mask policy in the data?

## Basic Code

```{r}
covid_us %>% 
  ggplot(aes(x=face_masks))+
  geom_bar(stat = "count")
```

## Better Code

```{r}
covid_us %>%
  ungroup() %>% 
  mutate(
    face_masks = forcats::fct_infreq(face_masks)
  ) %>% 
  ggplot(aes(x=face_masks,
             fill = face_masks))+
  geom_bar()+
  geom_text(stat='count', aes(label=..count..), 
            hjust=.5,vjust=-.5)+
  guides(fill = "none")+
  theme_bw()+
  labs(
    x = "Face Mask Policy ",
    title = ""
  ) -> fig_barplot
  
```

## Figure

```{r}
#| echo: false

fig_barplot
```
:::

## Histogram {.smaller}

::: panel-tabset
## Question

What does the distribution of new Covid-19 cases look like in June 2021

## Basic Code

```{r}
covid_us %>% 
  filter(year_month == "2021-06") %>% 
  ggplot(aes(x=new_cases))+
  geom_histogram() -> fig_hist1
```

## Ex 1

```{r}
fig_hist1
```

## Better Code

```{r}
covid_us %>%
  filter(year_month == "2021-06") %>% 
  filter(new_cases > 0) %>% 
  ggplot(aes(x=new_cases))+
  geom_histogram() +
  labs(
    title = "Exclude Negative Values"
  ) -> fig_hist2a

covid_us %>%
  filter(year_month == "2021-06") %>% 
  filter(new_cases > 0) %>% 
  ggplot(aes(x=new_cases))+
  geom_histogram() +
  scale_x_log10()+
  labs(
    title = "Exclude Negative Values & Use log scale"
  ) -> fig_hist2b

fig_hist2 <- ggarrange(fig_hist2a, fig_hist2b)

```

## Ex 2

```{r}
#| echo: false

fig_hist2
```
:::

## Density Plots {.smaller}

::: panel-tabset
## Question

What does the distribution of Covid-19 deaths look like?

## Basic Code

```{r}
covid_us %>% 
  mutate(
    new_deaths = deaths - lag(deaths),
    new_deaths_pc = deaths - lag(deaths)
  ) %>% 
  filter(new_deaths > 0) %>% 
  ggplot(aes(x=new_deaths_pc))+
  geom_density() -> fig_density1
```

## Ex 1

```{r}
fig_density1
```

## Better Code

```{r}
covid_us %>% 
  mutate(
    new_deaths = deaths - lag(deaths),
    new_deaths_pc = deaths - lag(deaths),
    year_f = factor(year)
  ) %>% 
  filter(new_deaths > 0) %>% 
  ggplot(aes(x=new_deaths_pc,
             col = year_f))+
  geom_density() +
  geom_rug() +
  scale_x_log10() +
    facet_wrap(~month)+
  theme(legend.position = "bottom")-> 
  fig_density2

```

## Ex 2

```{r}
#| echo: false

fig_density2
```
:::

## Box plots {.smaller}

::: panel-tabset
## Question

How did the distribution of Covid-19 cases vary by face mask policy?

## Basic Code

```{r}
covid_us %>%
  filter(new_cases_pc > 0) %>% 
  ggplot(aes(x= face_masks, y=new_cases_pc))+
  scale_y_log10()+
  geom_boxplot() -> fig_boxplot1
```

## Ex 1

```{r}
fig_boxplot1
```

## Better Code

```{r}
covid_us %>%
  mutate(
    Month = lubridate::month(date, label = T)
  ) %>% 
  filter(new_cases_pc > 0) %>% 
  filter(year == 2020) %>% 
 ggplot(aes(x= face_masks, 
            y=new_cases_pc,
            col = face_masks))+
  scale_y_log10()+
  coord_flip() +
  geom_boxplot()  +
    facet_wrap(~Month) +
  theme(
    legend.position = "bottom"
  )-> fig_boxplot2

```

## Ex 2

```{r}
#| echo: false

fig_boxplot2
```
:::

## Line graphs {.smaller}

::: panel-tabset
## Question

How did vaccination rates vary by state?

## Basic Code

```{r}
covid_us %>%
  ggplot(
    aes(x= date,
        y=percent_vaccinated,
        group = state
        ))+
  geom_line() -> fig_line1
```

## Ex 1

```{r}
fig_line1
```

## Better Code

```{r}

covid_us %>%
  ungroup() %>%
  mutate(
    Label = case_when(
      date == max(date) & percent_vaccinated == max(percent_vaccinated[date == max(date)], na.rm = T) ~ state,
      date == max(date) & percent_vaccinated == median(percent_vaccinated[date == max(date)], na.rm = T) ~ state,
      date == max(date) & percent_vaccinated == min(percent_vaccinated[date == max(date)], na.rm = T) ~ state,
      TRUE ~ NA_character_
    ),
    line_alpha = case_when(
      state %in% c("District of Columbia", "Nebraska", "Wyoming") ~ 1,
      T ~ .3
    ),
    line_col = case_when(
      state %in% c("District of Columbia", "Nebraska", "Wyoming") ~ "black",
      T ~ "grey"
    )
  ) %>%
  ggplot(
    aes(x= date,
        y=percent_vaccinated,
        group = state
        ))+
  geom_line(
    aes(alpha = line_alpha,
        col =line_col)) +
  geom_text_repel(aes(label = Label),
                  direction = "x",
                  nudge_y = 2) +
  guides(
    alpha = "none",
    col = "none"
  )+
  xlim(ym("2021-01"), ym("2023-01")) +
  labs(
    y = "Percent Vacinated",
    x = "Date"
  ) +
  theme_bw()-> fig_line2


```

## Ex 2

```{r}
#| echo: false

fig_line2
```
:::

## Scatterplots {.smaller}

::: panel-tabset
## Question

What's the relationship between vaccination rates and new cases of Covid-19?

## Basic Code

```{r}
covid_us %>%
  ggplot(
    aes(x= percent_vaccinated,
        y=new_cases_pc,
        ))+
  geom_point() -> fig_scatter1
```

## Ex 1

```{r}
fig_scatter1
```

## Better Code

```{r}

covid_us %>%
  filter(year > 2020) %>%
  filter(month == 6) %>%
  filter(new_cases_pc > 0) %>%
  ggplot(
    aes(x= percent_vaccinated,
        y=new_cases_pc,
        ))+
  geom_point() +
  geom_smooth(method = "lm")+
  facet_wrap(~year_month,ncol =1,
             scales = "free_y")-> fig_scatter2


```

## Ex 2

```{r}
#| echo: false

fig_scatter2
```
:::

# {{< fa home >}} Summary {.inverse}

## Summary

- The grammar of graphics provides a language for translating data into figures

- At a minimum figures with `ggplot()` require three things:
  - data
  - aesthetic mappings
  - geometries

- To produce a figure:
  - think about what the end product will look like
  - transform your data
  - map variables onto corresponding aesthetics
  - tell R what to do with these aesthetic mappings
  - Revise and iterate!
  
- Learning to code is hard, but the more errors you make now, the easier your life will be in the future
